---
title: "DataFramesObt"
author: "Diego Guerrero"
date: "11/3/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Filtro de tweets
```{r}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(readr)


names(tweetsAAPL)
subtweets <- subset(x = tweetsAAPL, like_num > 1)

subtweets <- subset(x = subtweets, Date > "2019-01-01")

recovery <- subtweets

summary(subtweets)
str(subtweets)

write.csv(subtweets , "2019_5l_Tweets.csv")

names(subtweets)
 # Checando dataframe 

nrApple <-read.csv("RAPPLE.csv")

nrApple

#Sacando para tesla 

tesla <- c("TSLA", "Tesla", "Elon Musk", "Stock")
tweetsTsla <- rawTweets[grep("Tesla ", rawTweets$body), ]

?grep


str(tweetsTsla)
summary(tweetsTsla)

tweetsLikeTesla <- subset(x = tweetsAAPL, like_num > 1)
str(tweetsLikeTesla)
summary(tweetsLikeTesla)

tweetsDateD <- subset(x = tweetsLikeTesla, Date > "2019-01-01") 
str(tweetsDateD)
summary(tweetsDateD)
  
write_csv(tweetsDateD, "tweetsTslaS.csv")


#Creamos dataframes de Noticias de Apple y Tesla 

allNews <-read.csv("us_equities_news_dataset.csv")

summary(allNews)

names(allNews)[6] <- "Date"

names(allNews)

allNews$Date <- as.Date(allNews$Date) #Hacemos un cohercing a Date 

class(allNews$Date) #checamos 

newsAaple <- subset(allNews, allNews$ticker == "AAPL")
summary(newsAaple)
newsAaple$Date <- as.Date(newsAaple$Date)

newsAapleDate <- subset(newsAaple, newsAaple$Date > "2018-12-31")
summary(newsAapleDate)
str(newsAapleDate)

newsAapleDate <- subset(newsAapleDate, newsAapleDate$Date < "2020-01-01") 
str(newsAapleDate)
summary(newsAapleDate)


#Tesla 


newsTesla <- subset(allNews, allNews$ticker == "TSLA")
summary(newsTesla)
newsTesla$Date <- as.Date(newsTesla$Date)

newsTeslaDate <- subset(newsTesla, newsTesla$Date > "2018-12-31")
summary(newsTeslaDate)
str(newsTeslaDate)

newsTeslaDateA <- subset(newsTeslaDate, newsTeslaDate$Date < "2020-01-01") 
str(newsTeslaDateA)
summary(newsTeslaDateA)

write.csv(newsTeslaDateA, "NoticiasTesla.csv")
write.csv(newsAapleDate, "NoticiasApple.csv")



```
# Creando Promedios (Time Series)
# Objetivo:
Crear una columna con el promedio de las polaridades de las fechas repetidas en una sola fecha.

```{r}

library(stringi)
library(tidyverse)
library(dplyr)
library(readr)


#Usaremos el dataset de nrAppl 

nrApple$Date <- as.Date(nrApple$Date) #Casting

repeatedApple <-nrApple #Guardamos otra variable con la base de datos con valores repetidos, por si acaso.

str(repeatedApple) # Revisamos estructura 
str(tweetsDateD)
summary(tweetsDateD)
summary(repeatedApple)

?duplicated

dup <- duplicated(nrApple$Date) #Creamos un vector lógico indicando que está repetido o no


nrApple$Duplicated <- dup #Agregamos este vector al dataset

str(nrApple)

RepeatedTweets <- subset(nrApple, nrApple$Duplicated == TRUE) # Hacemos un subset de los datos que se fije si es TRUE la columna "Duplicated" 

noRepeatedTweets <- subset(nrApple, nrApple$Duplicated == FALSE)#Hacemos un subset de los datos que se fije si es FLASE la columna "Duplicated" 


head(RepeatedTweets) #Chemos dataframe
str(RepeatedTweets)
view(RepeatedTweets)

head(noRepeatedTweets) #Checamos dataframe
str(noRepeatedTweets)
view(noRepeatedTweets)







#Generar dataframe de Tesla 

sixtNews <- read_csv("kaggleDB/raw_analyst_ratings(news).csv")

summary(sixtNews)

#aaplNews <- stri_locate_all(str = sixtNews$stock , regex = "AAPL")

sixtNews$date <- as.Date(sixtNews$date)

aaplNews<-subset(sixtNews, sixtNews$stock == "TSLA" )

tslaNews <- subset(aaplNews, aaplNews$date < "2020-01-01")

write_csv(tslaNews , "tslaNews.csv")



```

# Time Series 

```{r}
############



nrApple$Date <- as.Date(nrApple$Date) # Casting a la columna Date as.Date


summary(nrApple) #Checando la base de datos 

mergedTweets <- merge(nrApple, aaplClose) 

write.csv(mergedTweets, "mergedTweets.csv")


mergedTweets  %>% 
  ggplot( aes(x=Date, y=AAPL.Close) )+
  geom_line() 
  

mergedTweets  %>% 
  ggplot( aes(x=Date, y=Polarity) )+
  geom_line() 


```


# Promedio 
```{r}

library(stats)
library(readr)

promedio <- read.csv("promedios.csv")

str(promedio)
View(promedio)

promedio$Date <- as.Date(promedio$Date)

aaplClose

aaplMerged <- merge(promedio, aaplClose) #combinamos los data frames para obtener un dataframe maestro con precios, sentimiento y subjetividad  

write_csv(aaplMerged, "aaplMerged.csv")

# Se necesita sacarlos para la serie de tiempo 
aapl.Polaridad <- as.data.frame(aaplMerged$Polarity)
aapl.Precios <- as.data.frame(aaplMerged$AAPL.Close) 

class(aapl.Precios) #Les hacemos un casting a dataframe para poder generar la serie de tiempo 
class(aapl.Polaridad)

summary(aaplMerged) #Checamos las fechas para usarla en stats 

#Min.   :2019-01-02 , Max.   :2019-12-31

st.Precios.aapl <- ts(aapl.Precios , start = c(1,1), end = c(12, 12), frequency = 12 ) #Le damos formato de serie de tiempo 

st.Precios.aapl

?ts

class(st.Precios.aapl) #Checamos 

st.Precios.aapl

#Analisis de componentes

analisisComp.aaplPrice <- decompose(st.Precios.aapl)
plot(analisisComp.aaplPrice, 
     xlab = "Tiempo en meses",
     ylab = "Aleatorio, Estacional, Tendencia, Observada",
     cex.lab= 1)


```







*Ahora para las Polariodades*








```{r}
summary(aaplMerged) #Checamos las fechas para usarla en stats 

#Min.   :2019-01-02 , Max.   :2019-12-31

st.Polar.aapl <- ts(aapl.Polaridad , start = c(1,1), end = c(12, 12), frequency = 12 ) #Le damos formato de serie de tiempo 

?ts

class(st.Polar.aapl) #Checamos 

st.Polar.aapl

#Analisis de componentes

analisisComp.aaplPolar <- decompose(st.Polar.aapl)
plot(analisisComp.aaplPolar, 
     xlab = "Tiempo en meses",
     ylab = "Aleatorio, Estacional, Tendencia, Observada",
     cex.lab= 1)


```






## Análisis de cambios estructurales



## Precios 
```{r}
library(strucchange)


bp_appl_price <- breakpoints(st.Precios.aapl ~1) #Llamamos al comando breakpoints. 



?breakpoints

summary(bp_appl_price) #Revisamos 

plot(bp_appl_price)

```

```{r}

bp <- breakpoints(bp_appl_price, breaks = 3) #CAmbiar  a5 

ocus.appl <- efp(st.Polar.aapl ~ breakfactor(bp), type = "OLS-CUSUM")
plot(ocus.appl)

```

### Polaridad 

```{r}
library(strucchange)

bp_appl_polar <- breakpoints(st.Polar.aapl ~1) #Llamamos al comando breakpoints. 
?breakpoints

bp <- breakpoints(bp_appl_polar, breaks = 3)

ocus.appl <- efp(st.Polar.aapl ~ breakfactor(bp), type = "OLS-CUSUM")
plot(ocus.appl)



```


## Visualizando y localizando los breakpoints 




```{r}
summary(bp_appl_price)

# m = 1      47          
# m = 2   21 50          
# m = 3   21 49       121
# m = 4   21 51    90 111
# m = 5   21 47 68 90 111

bp_Price_vec <- c(21,47,50,49,51,47,68,90,121,111) #creamos un vector con las observaciones que tienen break points 

mergedAAPL$Trend_Break <- FALSE #Agregamos una columna que nos indique si el precio rompió la tendencia para usarlo en el modelo de regresión lineal. 

summary(mergedAAPL) #Confirmamos que se creó y que todos son falsos por default. 

#Ahora asignamos con el vector  de observaciones un TRUE a las que tienen bp


mergedAAPL[bp_Price_vec,]$Trend_Break <- TRUE

summary(mergedAAPL) #Ahora ya tenemos identificados los puntos de quiebre y en el mismo dataframe 


```

# Regresión 

Veremos si se relacionan nustros break points con la polaridad de las noticias 

```{r}
#Intento con precios 

summary(mergedAAPL)

covAppl <- cov(mergedAAPL$AAPL.Close, mergedAAPL$Polarity) #0.24 es una covarianza muy baja, inexistente 

stdevAppl <- sd(mergedAAPL$AAPL.Close, mergedAAPL$Polarity)

pearson.cor.Appl <- cor(mergedAAPL$AAPL.Close, mergedAAPL$Polarity)

pearson.cor.Appl #Muy baja correlación 


#Usando la columan Trend:break

#Creamos modelo de regresión lineal 

names(mergedAAPL)

trend_model <- lm(data = mergedAAPL, Trend_Break~Polarity + X)

#Lo llamamos

trend_model

summary(trend_model)

plot(trend_model)

trend_model

#Intento de regresiń binaria 
library(ISLR)

trend_model_bin <- glm(data = mergedAAPL, Trend_Break~Polarity + AAPL.Close , family = "binomial")

summary(trend_model_bin)

str(mergedAAPL)

#redondeo

mergedAAPL$Polarity_Round <- round(mergedAAPL$Polarity, digits = 1)

  

```

No hay relación, entonces intentaremos con un modelo de Supervised Machine Learning 

```{r}
#Aquí necesitamos el texto de los tweets. No su polaridad entonces tendremos que recurrir a la base de datos con los tweets de 2019

#Por esta vez volveremos a mezclar el dataset de tweets con los precios aunque se repitan, aquí ya no importa eso 

aapl.merged.bp <- merge(mergedAAPL, subtweets)

aapl.merged.bp #Se ve bien 

library(quanteda.textplots)
library(stringi)

summary(subtweets) #La checamos 
str(subtweets)

stri_replace(str = subtweets$body, regex = "$ABCD", replacement = "")
head(subtweets)

aapl.tweets.corpus<-corpus(x = aapl.merged.bp$body)

tokens(subtweets$body)

aapl.dtm <- dfm(aapl.tweets.corpus, stem = TRUE, remove = stopwords("english"), remove_punct= TRUE) #creamos dtm 

docvars(aapl.dtm)

#Particionamos en dataframe de training y testing 

set.seed(1)
summary(aapl.dtm)
train_dtm <- dfm_sample(aapl.dtm, size = 500)
setdiff(docnames(aapl.dtm), docnames(train_dtm))

test_dtm <-  aapl.dtm[setdiff(docnames(aapl.dtm), docnames(train_dtm)), ]

#Ahora creamos el modelo de Naive Bayes

nb_model <- textmodel_nb(train_dtm, y =  docvars(Trend_Break))



```













