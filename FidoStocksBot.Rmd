---
title: "RTWEET"
author: "Ivan Uriel Olvera Perez"
date: "25/8/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Vamos a probar la libreria de rtweet

```{r}
library(rtweet)
library(tidyverse)
#Ahora usaremos un comando para llamar una viñeta con un tutorial para vincular nuestra cuenta de twitter con R
vignette("auth", package = "rtweet")

```

```{r}
# Cargamos rtweet
library(rtweet)
library(tidyverse)
library(httpuv)
# Almacenamos nuestras llaves (aquí deben de poner sus llaves personales pero dejare las mías porque uds son de confianza)
api_key <- "e3hoiSSKA5Gkcoxo2FOQV4dw6"
api_secret_key <- "7Ys9RMOQiBjrxJETK8DJN9R8GKpAlCStbFkxjRGDtuB8ftji0q"

# 
token <- create_token(
  app = "FidoStocksBot",
  consumer_key = api_key,
  consumer_secret = api_secret_key)

#F a mi no me funciono este método, me marca error pero igual lo dejo si se quieren saltar el siguiente paso

```

```{r}
#Para este caso tenemos que generar nosotros directamente desde el portar de twitter developer nuestra token y secret token
library(rtweet)
library(httpuv)
api_key <- "e3hoiSSKA5Gkcoxo2FOQV4dw6"
api_secret_key <- "7Ys9RMOQiBjrxJETK8DJN9R8GKpAlCStbFkxjRGDtuB8ftji0q"
access_token <- "386220193-pR1NntJqcIaBTAZgmLSUoqYfa9yh6CWxV06a7FQI"
access_token_secret <- "L7z5cKSOib7bvb7YVhAl8JWSjAkYXWlZVxCJiLlyJRi05"

token <- create_token(
  app = "FidoStocksBot",
  consumer_key = api_key,
  consumer_secret = api_secret_key,
  access_token = access_token,
  access_secret = access_token_secret)

#Parece que funciono aunque pensé que me mandaría al navegador
```
Parece ser que ya tengo autorización probemos a buscar twits de messi
```{r}
rt <- search_tweets(q = "messi", n = 10)
rt
```

Ahora busquemos 500 twits sobre acciones de apple
```{r}
apple <- search_tweets(q = "apple and stocks", n = 500, include_rts = FALSE, retryonratelimit = FALSE)

#Veamos la tabla, aunque tenemos 90 columnas, así que tenemos que optimzar el comando para que nos lance unícamente las necesarias como texto, likes, usuario, seguidores del usuario y tal vez fecha; para así aumentar la veracidad del twit
head(apple)

```

Ya investigué como filtrar la información para solo tener la más útil, como fecha de creación, texto y hashtags

```{r}
Applecurado <- apple %>% dplyr::select(screen_name,created_at,text,hashtags,followers_count,favourites_count,verified)
#Guardamos nuestra base de datos curada en un csv 
save_as_csv(Applecurado, "/Users/TELECOM/Proyecto_EIII/Proyecto_EIII/TW_AAPL_CUR.csv", prepend_ids = TRUE, na = "", fileEncoding = "UTF-8")

```


Ahora usaremos otra función de rtweet para capturar twits en tiempo real acerca de las acciones de apple

```{r}
st <- stream_tweets(q = "covid19", timeout = 20)
st
#Me marca un error, despues vemos que show

```
Vamos a evaluar los hashtags acerca del mercado de valores para ver de que se está hablando

```{r}
StockMarket <- search_tweets(
  "finance market and stocks",n=200,include_rts = FALSE,retryonratelimit=F)
ht <- StockMarket %>% dplyr::select(created_at,screen_name,hashtags)


ht <- unnest(ht,cols=c(hashtags))

# Son muchos así que tomemos el top 20
top.20 <- ht %>% filter(is.na(hashtags)==F) %>% 
             group_by(hashtags) %>% summarise(count=n()) %>% 
               arrange(-count) %>% filter(row_number() < 21) %>% 
                 arrange(count) %>% mutate(hashtags=factor(hashtags,levels=hashtags))
```

Vamos a mostrarlos en una tabla 
```{r}
library(tidyr)
library(ggplot2)
library(ggthemes)
library(data.table)
ggplot(top.20,aes(x=hashtags,y=count)) + geom_bar(stat='identity') + 
  coord_flip() + theme_fivethirtyeight() + 
    ggtitle(label="Hashtags populares acerca del mercado financiero",
            subtitle="hashtag use last 10 days") +
     theme(plot.title = element_text(size = 14, face = "bold"),
           plot.subtitle = element_text(face = "italic"))
```
Vamos a crear y curar todas nuestras bases de datos 
TESLA
```{r}
tesla <- search_tweets(q = "tsla and stocks", n = 500, include_rts = FALSE, retryonratelimit = FALSE)

Teslacurado <- tesla %>% dplyr::select(screen_name,created_at,text,hashtags,followers_count,favourites_count,verified)
#Guardamos nuestra base de datos curada en un csv 
save_as_csv(Teslacurado, "/Users/TELECOM/Proyecto_EIII/Proyecto_EIII/TW_TSLA_CUR.csv", prepend_ids = TRUE, na = "", fileEncoding = "UTF-8")
```
GOLD
```{r}
GOLD <- search_tweets(q = "GOLD and stocks", n = 500, include_rts = FALSE, retryonratelimit = FALSE)

GOLDcurado <- GOLD %>% dplyr::select(screen_name,created_at,text,hashtags,followers_count,favourites_count,verified)
#Guardamos nuestra base de datos curada en un csv 
save_as_csv(GOLDcurado, "/Users/TELECOM/Proyecto_EIII/Proyecto_EIII/TW_GOLD_CUR.csv", prepend_ids = TRUE, na = "", fileEncoding = "UTF-8")
```
CRUDE OIL
```{r}
CRUDEOIL <- search_tweets(q = "OIL and stocks", n = 700, include_rts = FALSE, retryonratelimit = FALSE)

CRUDEOILcurado <- CRUDEOIL %>% dplyr::select(screen_name,created_at,text,hashtags,followers_count,favourites_count,verified)
#Guardamos nuestra base de datos curada en un csv 
save_as_csv(CRUDEOILcurado, "/Users/TELECOM/Proyecto_EIII/Proyecto_EIII/TW_CRUDEOIL_CUR.csv", prepend_ids = TRUE, na = "", fileEncoding = "UTF-8")
```
EUR/JPY
```{r}
EURJPY <- search_tweets(q = "EUR/JPY and forex", n = 10000, include_rts = FALSE, retryonratelimit = FALSE)

EURJPYcurado <- EURJPY %>% dplyr::select(screen_name,created_at,text,hashtags,followers_count,favourites_count,verified)
#Guardamos nuestra base de datos curada en un csv 
save_as_csv(EURJPYcurado, "/Users/TELECOM/Proyecto_EIII/Proyecto_EIII/TW_EURJPY_CUR.csv", prepend_ids = TRUE, na = "", fileEncoding = "UTF-8")
```
EURUSD
```{r}
EURUSD <- search_tweets(q = "EUR/USD and forex", n = 10000, include_rts = FALSE, retryonratelimit = FALSE)

EURUSDcurado <- EURUSD %>% dplyr::select(screen_name,created_at,text,hashtags,followers_count,favourites_count,verified)
#Guardamos nuestra base de datos curada en un csv 
save_as_csv(EURUSDcurado, "/Users/TELECOM/Proyecto_EIII/Proyecto_EIII/TW_EURUSD_CUR.csv", prepend_ids = TRUE, na = "", fileEncoding = "UTF-8")
```
USDMXN
```{r}
USDMXN <- search_tweets(q = "USDMXN AND FX", n = 1000, include_rts = FALSE, retryonratelimit = FALSE)

USDMXNcurado <- USDMXN %>% dplyr::select(screen_name,created_at,text,hashtags,followers_count,favourites_count,verified)
#Guardamos nuestra base de datos curada en un csv 
save_as_csv(USDMXNcurado, "/Users/TELECOM/Proyecto_EIII/Proyecto_EIII/TW_USDMXN_CUR.csv", prepend_ids = TRUE, na = "", fileEncoding = "UTF-8")
```
```{r}
library(rtweet)

#Con este comando nos da las tendencias mundiales en twitter
trends <- get_trends(woeid = 1)

```
```{r}
library(readr)
tweetsA <- read_csv("TW_AAPL_CUR.csv")
str(tweetsA)

```

```{r}
# Construir nuestro corpus
library(tm)
corpus <- iconv(tweetsA$text, to = "utf-8")
corpus <- Corpus(VectorSource(corpus))
inspect(corpus[1:5])

# Limpiar texto
corpus <- tm_map(corpus, tolower)
inspect(corpus[1:5])

corpus <- tm_map(corpus, removePunctuation)
inspect(corpus[1:5])

corpus <- tm_map(corpus, removeNumbers)
inspect(corpus[1:5])

cleanset <- tm_map(corpus, removeWords, stopwords('english'))
inspect(cleanset[1:5])

removeURL <- function(x) gsub('http[[:alnum:]]*', '', x)
cleanset <- tm_map(cleanset, content_transformer(removeURL))
inspect(cleanset[1:5])

cleanset <- tm_map(cleanset, removeWords, c('aapl', 'apple')) 
#Aquí eliminamos la palabra apple, ya que no nos interesa su frecuencia porque es la palabra filtrada para la búsqueda de los tweets
cleanset <- tm_map(cleanset, removeWords, c('stock', 'stocks'))
#Hacemos lo mismo con la palabra stock puesto que es la misma situación



cleanset <- tm_map(cleanset, stripWhitespace)
inspect(cleanset[1:5])

#matrix de términos del cdoc
tdm <- TermDocumentMatrix(cleanset)
tdm <- as.matrix(tdm)
tdm[1:10, 1:20]

# Bar plot
w <- rowSums(tdm)
w <- subset(w, w>=25)
barplot(w,
        las = 2,
        col = rainbow(50))

# Ahora crearemos una nube de palabras para que sea mas facil el analisis 
library(wordcloud)
w <- sort(rowSums(tdm), decreasing = TRUE)
set.seed(222)
wordcloud(words = names(w),
          freq = w,
          max.words = 150,
          random.order = F,
          min.freq = 5,
          colors = brewer.pal(8, 'Dark2'),
          scale = c(5, 0.3),
          rot.per = 0.7)

library(wordcloud2)
cloudw<- data.frame(names(w), w)
colnames(w) <- c('word', 'freq')
wordcloud2(w,
           size = 0.7,
           shape = 'triangle',
           rotateRatio = 0.5,
           minSize = 1)

letterCloud(w,
            word = "apple",
            size=1)


```

```{r}

# Sentiment analysis
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)

# Read file
apple <- read.csv(file.choose(), header = T)
tweetsA <- iconv(apple$text, to = 'utf-8')

# Obtain sentiment scores
s <- get_nrc_sentiment(tweetsA)
head(s)
get_nrc_sentiment('delay')

# Bar plot
barplot(colSums(s),
        las = 2,
        col = rainbow(10),
        ylab = 'Count',
        main = 'Sentiment Scores for Apple Tweets')
```

```{r}
getwd()
bd.tsla <- read.csv("TW_TSLA_CUR.csv")
str(bd.tsla)

```

```{r}
# Construir nuestro corpus
library(tm)
corpus2 <- iconv(bd.tsla$text, to = "utf-8")
corpus2 <- Corpus(VectorSource(corpus2))
inspect(corpus[1:5])

# Limpiar texto
corpus2 <- tm_map(corpus2, tolower)
inspect(corpus[1:5])

corpus2 <- tm_map(corpus2, removePunctuation)
inspect(corpus[1:5])

corpus2 <- tm_map(corpus2, removeNumbers)
inspect(corpus[1:5])

cleanset2 <- tm_map(corpus2, removeWords, stopwords('english'))
inspect(cleanset[1:5])

removeURL2 <- function(x) gsub('http[[:alnum:]]*', '', x)
cleanset2 <- tm_map(cleanset, content_transformer(removeURL))
inspect(cleanset[1:5])

cleanset2 <- tm_map(cleanset, removeWords, c('tsla', 'tesla'))
cleanset2 <- tm_map(cleanset, removeWords, c('stock', 'stocks'))


cleanset2 <- tm_map(cleanset2, stripWhitespace)
inspect(cleanset[1:5])

#matrix de términos del cdoc
tdm2 <- TermDocumentMatrix(cleanset2)
tdm2 <- as.matrix(tdm2)
tdm2[1:10, 1:20]

# Bar plot
z <- rowSums(tdm2)
z <- subset(z, w>=25)
barplot(z,
        las = 2,
        col = rainbow(50))

# Ahora crearemos una nube de palabras para que sea mas facil el analisis 
library(wordcloud)
z <- sort(rowSums(tdm2), decreasing = TRUE)
set.seed(222)
wordcloud(words = names(z),
          freq = z,
          max.words = 150,
          random.order = F,
          min.freq = 5,
          colors = brewer.pal(8, 'Dark2'),
          scale = c(5, 0.3),
          rot.per = 0.7)

library(wordcloud2)
cloudw2<- data.frame(names(z), z)
colnames(z) <- c('word', 'freq')
wordcloud2(z,
           size = 0.7,
           shape = 'triangle',
           rotateRatio = 0.5,
           minSize = 1)

letterCloud(z,
            word = "tesla",
            size=1)

```
```{r}
view(z)
```


