---
title: "Preparing_Data"
author: "Diego Guerrero"
date: "9/25/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
#Libraries
libs <- c('rtweet','quantmod','tidyverse','dyplr')
library(rtweet)
library(quantmod)
library(tidyverse)
library(stringi)
library(quanteda)


```
# Limpieza
*Al parecer nuestros tweets y los precios no tinen ningúna fecha en común, se me ocurre jalar más tweets. Sin embargo normalmente hay varios tweets en un día, por lo tanto tendría que combinar la database con el sentiment analysis pero tendría que hacer las observaciones por día.*
```{r}

#Extraemos los datos de twitter y nos aseguramos que la columna de created_at sea caracter para poder pasarlo a Date

  
TW_AAPL_CUR <- search_tweets("Apple stock",n = 15000, include_rts = FALSE,lang = "en")
View(TW_AAPL_CUR)
str(TW_AAPL_CUR)
summary(TW_AAPL_CUR)
names(TW_AAPL_CUR)
#Al parecer sólo hay 820
TW_AAPL_CUR <- select(TW_AAPL_CUR,created_at,text,followers_count,verified)
TW_AAPL_CUR$Date <- as.character(TW_AAPL_CUR$Date)

View(TW_AAPL_CUR)

#Cambiamos created_at por Date

names(TW_AAPL_CUR)[1] <- "Date"
names(TW_AAPL_CUR)


```

*Como en la base de datos de precios no tenemos la hora, sólo la fecha. Tendremos que eliminar la hora para poder hacer la serie temporal*

```{r}

# Le decimos que desde el caracter 11 al 19 de la columna de "Date" los reemplace por "         ", es importante notar que estos espacios actúan como un corrector, tienen que ser el mismo número que los caracteres, sino, no alcanzarán a taparlos. 

substring(TW_AAPL_CUR$Date,11,19) <- "          "
View(TW_AAPL_CUR)
str(TW_AAPL_CUR)

#En este punto ya tapamos la hora, lo que sigue es eliminar esos espacios usando str_remove_all, le pasamos la columna como la strng y en el patrón ponemos " " para que busque por espacios. 

#Eso lo pasamos como argumento a as.Date para que lo ponga en formato de fecha. 

TW_AAPL_CUR$Date <- as.Date(str_remove_all(TW_AAPL_CUR$Date, " "))
str(TW_AAPL_CUR)
#Ahora tenemos una columnda de Date que es compatible con la columna Date de la DB de precios. 
View(TW_AAPL_CUR)


```
## Limpieza de stocks

```{r}
#Recuperamos los precios del csv
AAPL_stock<-getSymbols(c("AAPL"))

#Cambiamos la primer columna a Date
names(AAPL)

#Creamos un dataframe para poder utilizarlo con la mayoría de funciones en R, sin embargo dejamos AAPL ya que puede usarse para sacarle mucho jugo con el paquete de quantmod, como el uso de indicadores.
AAPL_sdf <- data.frame(AAPL) 


#Checamos
names(AAPL)

#Las fechas las detecta como numerador de renglones, entonces agregamos una nueva columna con estos para poder usarla
#Lo pasamos como fecha con as.Date y así nos ahorramos convertirla después. 
AAPL_sdf$Date <- as.Date(row.names(x = AAPL_sdf))
View(AAPL_sdf)
str(AAPL_sdf)


```
# Haciendo las Bases de datos compatibles

```{r}
#Realmente los usuarios no son importantes
TW_AAPL_CUR <-select(TW_AAPL_CUR, -screen_name)
names(TW_AAPL_CUR)
View(TW_AAPL_CUR)
str(TW_AAPL_CUR)

#Ahora creamos una nueva data frame, los precios y los tweets estarán en una misma fila si es que coinciden las fechas 

M_AAPL <-merge(TW_AAPL_CUR, AAPL_sdf, by.x = "Date", by.y = "Date")
View(M_AAPL)
#Eliminamos .AAPL pues estamos hablando de AAPL. Para eso usamos la función str_remove. 
names(M_AAPL) <- str_remove(names(M_AAPL), "AAPL.") 
View(M_AAPL)


#realmente solo nos interesa el precio de cierre entonces podemos desechar las demás columnas y 


names(M_AAPL)
CM_AAPL <- select(M_AAPL, Date, text, followers_count, verified,Close)


View(CM_AAPL)
ggplot()

```


# Quanteda Procesamiento de lenguaje Natural 

Para crear la DTM necesitamos primero crear el corpus. Ya con el corpus creamos la DTM pero primero tenemos que realizar **steming**, esto se refiere a agrupar plurales y letras capitalizadas como uno sólo para crear un concepto, ejemplo: gato, gatos, Gato se refieren al animal gato pero para la computadora son diferentes; para eso es el **steming**. DEspués evitamos las palabras que no crean un concepto como adjetivos, en Inglés se le llaman **stopwords**. 

```{r warning=TRUE}
#Creamos el corpus: Primer argumento es la base de datos, con text_field indicamos en qué columna se encuentra el texto 

corpAAPL = corpus(CM_AAPL, text_field = 'text')

#Creamos la DTM 

tokAAPL = tokens(x = corpAAPL, remove_url = T, remove_punct = T) #Priemro tenemos que tokenizar, aquí removemos urls y puntuaciones

dtmAAPL = dfm(x = tokAAPL, stem=T) # Ahora si creamos la dtm con el token, y ponemos stem = T para stemming

#Creo que tenemos que quitar las palabras que usamos para obtener los tweets como Apple, stock ya las usamos para generar los tweets y obviamente el algoritmos las clasificará como palabras que se repiten en todos los casos, esto es redundante porque eso ya lo sabemos. Y obviamente las stopwords

dtmAAPL <- dfm_remove(dtmAAPL, stopwords('english') ) #Quitamos las stopwords

redundant <- c('apple','appl','stock', 'stockmarket','aapl')

dtmAAPL <- dfm_remove(dtmAAPL, redundant) #Ahora quitamos las palabras redundantes






#Checamos

View(dtmAAPL) 
str(dtmAAPL)


#Al realizar la dtm quita las fechas, por eso usamos los tweets que ya fueron emparejados con los precios de las stocks. No pierden el orden, de esta manera podemos saber la fecha de un texto con su número. 

dtmAAPL # Vemos las estadísticas. 

# Si tuviéramos más observaciones sería bueno conservar sólo las que se repiten más de diez veces, pero como estamos restringidos por rtweet lo dejamos aśi. 



```

# Es hora de plotear


```{r}
#POr alguna razón quanteda como que se fragmentó, entponces tenemos que implementar más librerías
library(quanteda.textplots)
library(quanteda.textmodels)
library(quanteda.textstats)

textplot_wordcloud(dtmAAPL, max_words = 50) #Aparece un emoji, el emoji de la flecha abajo roja indicando una baja en los precios jajaja puede ser usado en el análisis
textplot_wordcloud(dtmAAPL, max_words = 100, color = c('blue', 'magenta', 'red'))

#Hay dos cosas que no me gustan, los hashtags hacen que se repitan los conceptos y no son detectados por el stemming y la palabra stock sigue apareciendo. 

#Creo que tendré que modificar la CM_AAPL para quitar los caracteres # 

View(CM_AAPL)
CM_AAPLrp <- CM_AAPL
?str_replace
CM_AAPL$text <- str_remove_all(string = CM_AAPL$text, pattern = "#") 
View(CM_AAPLrp)

#Regresar a volver a cargar los tokens ^^^
#Ahora si lso hashtags se eliminaron



```

# Analisis

## Rule based
Busca keywords para decir si es bueno o malo. El humano pone las reglas, que buscamos y como lo buscamos. Se usan diccionarios, esta palabra tiene un valor sentimental de 1.23 y así. 

 
```{r}
myDict <- dictionary(list(terror = c('terror*'), economy = c('job', 'business', 'econom*')))

dict_dtm <- dfm_lookup(dtmAAPL, myDict, nomatch = '_unmatched')

dict_dtm



```


## Supervised machine Learning

Se usan algoritmos como Naive Bayes, SVM y deep neural networks
Necesitamos resultados ya etiquetados. En otras palabras necesitamos textos ya etiquetados como malos o buenos. 
Ejemplo: 
```{r}
library(quanteda.textmodels)
CM_AAPL



```



## Unsupervised Machine Learning
Entrenan un modelo pero busca nuevos patrones 

- Factor analysis
- Topic modeling: 
    Busca temas. 
    Latent Dirichlet Allocation (LDA)
      Cada palabra es clasificada con un tema. 
  Ejemplo: 
```{r}
#Aqui es un poco complicado, para instalar el paquete topicmodels se necesita gsl (gnu scientific library), en mi sistema fedora la única forma en la que lo logré fue con el siguiente comando de CENTOS: 

#sudo yum install gsl-devel

library(quanteda)
library(quanteda.textmodels)
library(topicmodels)

text 

```
  
      
- Scaling 
 *IDEA*

Se me ocurrió que podemos usar alguna función de quantmod para que detectara cuando hay un cambio en la trend, añadirlo en una columna si es que hubo un cambio y usar esa columna con supervised Machine Learning 


## Preparando nueva columna con Quantmod

```{r}
library(quantmod)
library(rtweet)

getSymbols()
?getSymbols

#No puedo utilizar los indicadores de quantmod en un dataset normal, necesito usar el tipo de objeto que regresa la función getSymbols()
#Para eso extraigo del objeto original AAPL las observaciones que concuerden con las que están en el dataset de los precios con los tweets. 


AAPL_date <- AAPL[CM_AAPL$Date] #De esta forma no modifico el tipo de objeto que es, sólo el contenido 

summary(AAPL_date) #Podemos ver que las fechas concuerdan con las del dataset de tweets y precios 
AAPL_date

#Sin embargo si le digo que seleccione las observaciones del dataset de tweets con precios van a salir fechas repetidas, tal vez si sólo me fijo en el límite inferior y superior de las fechas se pueda plotear algo mejor. 

class(AAPL)
class(AAPL_date)

#Creo un vector que contenga una secuencia de fechas que use la mínima y máxima fecha de CM_AAPL.  

dates<- seq(min(CM_AAPL$Date), max(CM_AAPL$Date), by='days')
dates

#Al parecer son muy pocos días, sólo 8 



AAPL_schart <- chart_Series(x = AAPL, subset = dates)

AAPL_schart 







```

*En este punto me doy cuenta que son muy pocos tweets, necesito más*


# Sentiment analysis 
Quanteda.sentiment es una extensión del paquete quanteda. Este paquete realiza un sentiment analysis de dos formas: Asignando polaridades de positivo y negativo, o bien etiqueta con valencias cierto grado de algun sentimiento en el texto. Creo que la segunda nos ayuda más para nuestro objetivo. 

El link del repo de la extensión: https://github.com/quanteda/quanteda.sentiment/

De ahí me basé para realizar esta parte. 

```{r}


devtools::install_github("quanteda/quanteda.sentiment") #Instalamos la extensión 

library("quanteda", warn.conflicts = FALSE, quietly = TRUE) #No sé por que agregamos esos parámetros pero los pongo por si acaso 
library("quanteda.sentiment")

#Presentamos el diccionario que usaremos, este contiene palabras que han sido etiquetadas niveles de los sentimientos 
#pleasure, arousal, dominance

print(data_dictionary_ANEW, max_nval = 8) #El argumento max_nval es el máximo de variables que se imprimen en la pantalla


names(data_dictionary_ANEW)
#También está el diccionario AFINN el cual creo que será un poco más útil pues sólo tiene un rango de -5 a +5 de positividad
names(data_dictionary_AFINN)
#Por el momento seguiré con el ANEW
lapply(valence(data_dictionary_ANEW), head, 8) 

#Un ejemplo con un texto proveído por el paquete
tail(data_corpus_inaugural) %>% 
  textstat_valence(dictionary = data_dictionary_ANEW['pleasure'])

#Utilizando nuestros tweets con AFFIN 

tail(corpAAPL) %>% 
  textstat_valence(dictionary = data_dictionary_AFINN)

#Utilizando polarizado con nuestros tweets

print(data_dictionary_geninqposneg, max_nval = 8) #Divide los sentimientos en positivos y en negativos 



polAAPL <-  textstat_polarity(corpAAPL, dictionary = data_dictionary_geninqposneg) #0 es negativo y 1 es positivo 

texts

?ggplot()

ggplot(polAAPL, aes(x = doc_id, y = sentiment))+
  geom_smooth()

names(CM_AAPL)
ggplot(CM_AAPL, aes(x = Date, y = Close))+ geom_smooth()

View(polAAPL)
names(polAAPL)
#Me doy cuenta que el tener varias observaciones repetidas hace que el grafico no se vea bien. Intentare combinarlas y sacar un valor promedio del sentimiento
CM_AAPL
wCM_AAPL <- pivot_wider(data = CM_AAPL, id_cols = AAPL)
wCM_AAPL <- reshape(data = CM_AAPL, direction = "wide" ,v.names ="text", timevar = "followers_count", idvar = "Date")


wCM_AAPL[3:4]
wCM_AAPL[4] <-   paste(wCM_AAPL[4],wCM_AAPL[5])
names(wCM_AAPL)

names(wCM_AAPL)

View(wCM_AAPL)


AAPL_db <- read_csv("AAPL_db.csv") 
View(AAPL_db)
names(AAPL_db)
names(AAPL_db)[1] <- "Date"

TW_AAPL

AAPL_db

?gather
?pivot_longer()

## Graficamos
ggplot(head(AAPL_db), aes(x=Date, y=AAPL.Close) )+
geom_abline() 

grafico <- AAPL_db  %>% 
  ggplot( aes(x=Date, y=AAPL.Close) )+
  

ggplotly(grafico)

```
